## 0. 概念：   

循环神经网络（Recurrent Neural Network，RNN）是在Transformer架构出现之前，处理序列问效果最好的网络结构。

序列问题，简单来说，就是处理**数据之间存在顺序关系**的问题，数据的顺序改变则代表含义改变。更正式一点的定义是：

序列问题是指输入、输出或两者都是有序的数据序列，元素之间存在时间上或位置上的依赖关系，模型在处理当前元素时需要考虑前面（或未来）元素的信息。
序列问题的特点是：

- 数据是按顺序排列的（顺序有意义）。
    
- 数据之间存在依赖关系（后面的值依赖前面的值）。
    
- 进行处理时不能简单打乱数据顺序。

典型的序列问题有：

- 自然语言处理（NLP）
    
- 时间序列预测（股价、天气）
    
- 语音识别
    
- 视频分析
    
- 机器人控制
## 1. 序列到序列的问题

以之前讲过的NLP相关问题为例，可以发现，序列到序列问题有以下几种输入和输出情况：

**多对一**

比如输入为一条评论文本，输出是一个二分类，判断是评论正面还是负面情绪。输入是多个向量，输出是一个向量。

**多对多**

多对多任务分为两种，一种是输入token序列长度和输出token序列长度完全一样。比如命名实体识别（NER）任务。还有一种是输入token序列和输出token序列长度不一样的情况，比如对输入文本生成摘要，或者翻译任务。输入和输出都是多个向量。

不论对于输入或输出的序列而言，它们的序列元素个数也是不固定的，我们不能像之前设计全连接神经网络那样，固定输入和输出的神经元个数。
### 13.1.2 RNN网络结构

我们还是以之前举的NER任务为例，目标是识别输入句子里的人名。我们分别看两个输入：

_“我的鞋是李宁的。”_

_“我叫李宁。”_

一个好的NER模型应该能根据上下文识别出第一句里的“李宁”是品牌名，第二句的“李宁”是人名。

为了让RNN能够处理变长的输入，它被设计成循环调用的方式，每次只输入序列里的一个元素。对应到NLP任务里，每个元素就是一个token的embedding。

以第二个句子“我叫李宁”为例，目标是识别人名，所以对于每个token的输出是一个三分类（B-N：名字的开头，I-N：名字的后续，O：其他）。NER的输入Token序列长度和输出序列长度一样。

![[rnn结构.png]]




RNN对于第一个token，输入是它的embedding，经过两层，一个隐藏层（3个神经元）。一个是输出层，输出层有3个神经元，对应NER的三分类。这看起来和普通的神经网络没有区别。

RNN需要为序列保存记忆，所以上一层隐藏层的输出，会和当前层token的embedding合并，作为当前层的输入。对于第0层，可以输入一个同纬度的全零记忆，表示之前没有记忆，并统一格式。

因为当前层输入只和当前输入token和前一层的隐藏层输出有关，如果输入信息太长，前后关联性就会降低，从而引入LSTM。

用公式详细定义一下RNN的计算：

用$\mathbf{x_t}$​表示第t个时刻的输入。

用$\mathbf{y_t}$表示第t个时刻的输出。

用$\mathbf{h_t}$​表示第t个时刻更新后记忆，更常把它叫做隐状态。

用$\mathbf{w_h}$​表示隐藏层的权重，用$\mathbf{b_h​}$表示隐藏层的偏置。

用$\mathbf{w_y}$​表示输出层的权重，用$\mathbf{b_hy}$​表示输出层的偏置。

当前时刻的隐状态等于前一时刻的隐状态和当前时刻的序列输入进行拼接，然后经过隐藏层线性变化和激活函数后的输出。

$$
\boldsymbol{h_t} = \tanh\big([\boldsymbol{h_{t-1}} \mid \boldsymbol{x_t}] \mathbf{w_h} + \mathbf{b_h} \big)
$$
这里一般更常用tanh作为激活函数，也可以用ReLU。之前我们介绍过ReLU可以更好的缓解梯度消失，RNN用tanh做激活，是因为它有其他方法来应对梯度消失。

输出层是一个多分类，激活函数用softmax：

$$
y_t = \text{softmax}(h_t \mathbf{w_y} + \mathbf{b_y})
$$


