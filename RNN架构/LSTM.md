### 作用：解决长期记忆依赖


## RNN中的问题

从前向传播来看，每一个时间步的输入都是由上一个时间步的隐状态和当前时间步的输入一起进入一个线性层和激活函数，而复杂的线性变化和激活函数会让过久的隐状态难以保留。
从后向传播来看，如果需要的信息的隐状态在传播途中被意外丢失，那么模型就会”胡言乱语“。
![[长距离反向传播.png]]

## LSTM网络结构

![[LSTM.png]]

