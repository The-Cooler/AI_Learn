### 作用：解决长期记忆依赖


## RNN中的问题

从前向传播来看，每一个时间步的输入都是由上一个时间步的隐状态和当前时间步的输入一起进入一个线性层和激活函数，而复杂的线性变化和激活函数会让过久的隐状态难以保留。
从后向传播来看，如果需要的信息的隐状态在传播途中被意外丢失，那么模型就会”胡言乱语“。
![[长距离反向传播.png]]

## LSTM网络结构

![[LSTM.png]]

### 图解：

#### $W_{h} W_{i} W_{f} W_{o}$：
线性回归的权重参数
#### ${Z}$：
输入信息为上一时间步的隐状态+当前时间步的输入。新信息经过激活函数tanh后，通过输入门控制哪些维度可以写入长期记忆。
公式表达为：
$$Z = [h_{t - 1} | X_{t}] W_{h} + b_{h}$$
#### $Z_{i}$： 
门函数sigmoid输出是一个取值\[0-1\]向量，对新信息的每个维度单独控制。
公式表达为：
$$Z_{i} = [h_{t - 1} | X_{t}] W_{i} + b_{i}$$
$$G_{i} = sigmoid(Z_{i})$$
#### $Z_{f}$：
遗忘门同样使用一个取值\[0-1\]的向量，通过和记忆细胞的长期记忆按位乘运算后，遗忘长期记忆的某些维度信息。
公式表达为：
$$Z_{f} = [h_{t - 1} | X_{t}]W_{f} + b_{f}$$
$$G_{f} = sigmoid(Z_{f})$$

#### $Z_{o}$：
输出门通过记忆细胞通过和激活函数tanh的输出按位乘后，得到输出隐状态。
公式表达为：
$$Z_{o} = [h_{t - 1} | X_{t}]W_{o} + b_{o}$$
$$G_{o} = sigmoid(Z_{o})$$
#### 记忆细胞状态：
输入门过滤信息和遗忘门过滤信息的按位加。
公式表达为：
$$c_{t} = c_{t - 1} \odot G_{f} + tanh(Z) \odot G_{i}$$
#### 隐状态：
公式表达为：
$$h_{t} = tanh(c_{t}) \odot G_{o}$$